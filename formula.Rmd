---
title: "formua"
output: pdf_document
---

Table of Contents
-----------------------
Analysis of Old Formula
New Formula
Tests
Plots


Analysis of Old Formula
-----------------------
```{r}
options(warn=-1)
suppressPackageStartupMessages(library(data.table))
suppressPackageStartupMessages(library(bit64))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(tidyr))
suppressPackageStartupMessages(library(ggplot2))

if (getwd()=="C:/stuff/stanford") {
  setwd("C:/stuff/stanford/alignment")
}
if(getwd()=="/Users/jakeprasad"){
  setwd("/Users/jakeprasad/alignment")
}
```



It seems our plots aren't giving us expected results. Let's try and figure out why that may be

We know that smoothing is affecting our plots - especially because it affects one term more than another.  Maybe instead of subtracting logs, we should move everything inside of the log?

Let's do some thinking math...

For a completely shuffled dataset, we expect alignment to be equal to 0. Let's check if the formula gives us that result.

(A bunch of work done on paper)

This is interesting. It seems that whether alignment is positive or negative depends on if 

(ba\*nbna) is less than (nba\*nba). Let's check what happens in our shuffled datas

```{r}
  df <- fread('debug/shuffled/TTTTTTTT.csv', header=T)

  test <- ((df$ba*df$nbna)/(df$bna*df$nba))
  test[is.infinite(test)] <- NA
  mean(test, na.rm=T)
  
  df <- fread('debug/shuffled/TTTTTTTT1.csv', header=T)

  test <- ((df$ba*df$nbna)/(df$bna*df$nba))
  test[is.infinite(test)] <- NA
  mean(test, na.rm=T)
  
  df <- fread('debug/shuffled/TTTTTTTT2.csv', header=T)

  test <- ((df$ba*df$nbna)/(df$bna*df$nba))
  test[is.infinite(test)] <- NA
  mean(test, na.rm=T)
```

Hmm, we're consistently getting a <1 value, indicating a negative alignment. How do we fix this?

Let's do some more thinkging about what we're trying to calculate. We want to check whether the probability of B saying a given marker is more immediately after A says a given marker. 

We're getting a negative alignment on shuffled data because the base frequencies of markers is small.

Let's take a look at the numbers

df$ba ~= 0
df$nbna ~= 1

df$bna ~= >0, <1
df$nba ~= >0, <1

Since ba*nbna is always going to be approximately 0, we're getting the <1 result.

More thinking...

New Formula
-------------------------------------------------

Let's rethink the formula with examples

If A influences B, and A says the marker a lot
Before: Afreq = 1, Bfreq = 0
After: Afreq = 0.9, Bfreq = 0.5

If B influences A, and A says the marker a lot
Before: Afreq = 1, Bfreq = 0
After: Afreq = 0.5, Bfreq = 0.1

If A influences B, and A doesn't say the marker a lot
Before: Afreq = 0, Bfreq = 1
After: Afreq = 0.1, Bfreq = 0.5

If B influences A and A doesn't say the marker a lot
Before: Afreq = 0, Bfreq = 1
After: Afreq = 0.5, Bfreq = 0.9

How do we mathematically compare the freqs?

If A influences B, and A says the marker a lot
P(A|nB utterance): Afreq = 1
P(B|nA utterance): Bfreq = 0
P(A|B utterance): Afreq = 0.9
P(B|A utterance): Bfreq = 0.5

If B influences A, and A says the marker a lot
P(A|nB utterance): Afreq = 1
P(B|nA utterance): Bfreq = 0
P(A|B utterance): Afreq = 0.5
P(B|A utterance): Bfreq = 0.1

If A influences B, and A doesn't say the marker a lot
P(A|nB utterance): Afreq = 0
P(B|nA utterance): Bfreq = 1
P(A|B utterance): Afreq = 0.1
P(B|A utterance): Bfreq = 0.5

If B influences A and A doesn't say the marker a lot
P(A|nB utterance): Afreq = 0
P(B|nA utterance): Bfreq = 1
P(A|B utterance): Afreq = 0.5
P(B|A utterance): Bfreq = 0.9


And now we don't need to know relative base freqs

If A influences B
P(A|nB utterance): Afreq = 1
P(B|nA utterance): Bfreq = 0
P(A|B utterance): Afreq = 0.9
P(B|A utterance): Bfreq = 0.5

If B influences A
P(A|nB utterance): Afreq = 1
P(B|nA utterance): Bfreq = 0
P(A|B utterance): Afreq = 0.5
P(B|A utterance): Bfreq = 0.1

If A influences B
P(A|nB utterance): Afreq = 0
P(B|nA utterance): Bfreq = 1
P(A|B utterance): Afreq = 0.1
P(B|A utterance): Bfreq = 0.5

If B influences A
P(A|nB utterance): Afreq = 0
P(B|nA utterance): Bfreq = 1
P(A|B utterance): Afreq = 0.5
P(B|A utterance): Bfreq = 0.9




And now the coup d'etat

If A influences B
P(A|B) - P(A|nB) -  = 0.9 - 1  = -0.1 = B's influence on A
P(B|A) - P(B|nA) -  = 0.5 - 0 = 0.5 = A's infleunce on B

If B influences A
P(A|nB) - P(A|B) = 1 - 0.5 = 0.5 = B's influence on A
P(B|nA) - P(B|A) = 0 - 0.1 = -0.1 = A's influence on B

If A influences B
P(A|nB) - P(A|B) = 0 - 0.1 = -0.1 = B's influence on A
P(B|nA) - P(B|A) = 1 - 0.5 = 0.5 = A's influence on B

If B influences A
P(A|nB) - P(A|B) = 0 - 0.5 = -0.5 = B's influence on A
P(B|nA) - P(B|A) = 1 - 0.9 = 0.1 = A's influence on B

Therefore, if 
abs(P(B|A) - P(B|nA)) > abs(P(A|B) - P(A|nB)), A influences B

Let's see what the formula looks like:

P(A|nB) = anb/(anb+nanb)

P(A|B) = ab/(ab+nab)

P(B|nA) = bna/(bna+nbna)

P(B|A) = ba/(ba+nba)

Therefore,

abs(ba/(ba+nba) - bna/(bna+nbna)) - abs(ba/(ba+bna) - nba/(nba+nbna))

And after log spacing

log(abs(ba/(ba+nba) - bna/(bna+nbna))) - log(abs(ba/(ba+bna) - nba/(nba+nbna)))




Tests
-------------------------

Let's think about how it'd work on shuffled data with example numbers:

1) A says the marker the same number of time B says the marker
=> P(a|nb) == P(b|na) == nba/(nba+nbna) - bna/(bna+nbna) == 0
2) A does not influence B
=> P(b|a) == P(b|na) == ba/(ba+nba) - bna/(bna+nbna) == 0
3) We're using valid frequencies
ba + nba + bna + nbna == 1

```{r}
  ba = 0.25
  nba = 0.25
  bna = 0.25
  nbna = 0.25

  nba/(nba+nbna) - bna/(bna+nbna) == 0
  ba/(ba+nba) - bna/(bna+nbna) == 0
  ba + nba + bna + nbna == 1
  
  log(abs(ba/(ba+nba) - bna/(bna+nbna))) - log(abs(ba/(ba+bna) - nba/(nba+nbna))) == 0
  
```

Interesting...we're getting NA because all the frequencies equal to each other. Smoothing is discussed two sections later. Let's skip the shuffle case till then.


How about for an expected positive alignment?

Let's say that A influences B more than B influences A. And let's also suppose A says the marker a lot and B doesn't. There are two things to take into acount here:

1) A says the marker more than B says the marker
=> P(a|nb) > P(b|na) == nba/(nba+nbna) - bna/(bna+nbna) > 0
2) A influences B
=> P(b|a) > P(b|na) == ba/(ba+nba) - bna/(bna+nbna) > 0
3) We're using valid frequencies
ba + nba + bna + nbna == 1

  
```{r}
  ba = 0.3
  nba = 0.4
  bna = 0.1
  nbna = 0.2

  nba/(nba+nbna) - bna/(bna+nbna) > 0
  ba/(ba+nba) - bna/(bna+nbna) > 0
  ba + nba + bna + nbna == 1
  
  log(abs(ba/(ba+nba) - bna/(bna+nbna))) - log(abs(ba/(ba+bna) - nba/(nba+nbna))) > 0
  
```

Positive alignment =)



Let's say that A influences B more than B influences A. And let's also suppose B says the marker more than A does:

1) A says the marker less than B says the marker
=> P(b|na) > P(a|nb)  ==  bna/(bna+nbna) - nba/(nba+nbna) > 0
2) A influences B
P(b|na) > P(b|a)  == bna/(bna+nbna) - ba/(ba+nba) > 0
3) We're using valid frequencies
ba + nba + bna + nbna == 1

```{r}
  ba = 0.05
  nba = 0.1
  bna = 0.70
  nbna = 0.15

  bna/(bna+nbna) - nba/(nba+nbna) > 0
  bna/(bna+nbna) - ba/(ba+nba) > 0
  ba + nba + bna + nbna == 1
  
  log(abs(ba/(ba+nba) - bna/(bna+nbna))) - log(abs(ba/(ba+bna) - nba/(nba+nbna))) > 0
  
```

Positive alignment =)



Let's say that B influences A more than A influences B. And let's also suppose B says the marker more than A does:

1) B says the marker more than A says the marker
=> P(b|na) > P(a|nb)  ==  bna/(bna+nbna) - nba/(nba+nbna) > 0
2) B influences A
P(a|b) > P(a|nb)  == ba/(ba+bna) - nba/(nba+nbna) > 0
3) We're using valid frequencies
ba + nba + bna + nbna == 1

```{r}
  ba = 0.3
  nba = 0.1
  bna = 0.3
  nbna = 0.3

  bna/(bna+nbna) - nba/(nba+nbna) > 0
  ba/(ba+bna) - nba/(nba+nbna) > 0
  ba + nba + bna + nbna == 1
  
  log(abs(ba/(ba+nba) - bna/(bna+nbna))) - log(abs(ba/(ba+bna) - nba/(nba+nbna))) < 0
  
```

Negeative alignment =)



Let's say that B influences A more than A influences B. And let's also suppose A says the marker more than B does:

1) A says the marker more than B says the marker
=> P(a|nb) > P(b|na) == nba/(nba+nbna) - bna/(bna+nbna) > 0
2) B influences A
P(a|nb) > P(a|b) == nba/(nba+nbna) - ba/(ba+bna) > 0
3) We're using valid frequencies
ba + nba + bna + nbna == 1

```{r}
  ba = 0.1
  nba = 0.5
  bna = 0.1
  nbna = 0.3

  nba/(nba+nbna) - bna/(bna+nbna) > 0
  nba/(nba+nbna) - ba/(ba+bna) > 0
  ba + nba + bna + nbna == 1
  
  log(abs(ba/(ba+nba) - bna/(bna+nbna))) - log(abs(ba/(ba+bna) - nba/(nba+nbna))) < 0
  
```

Negeative alignment =)


Smoothing
--------------------
Let's talk about smoothing. Ideally, we don't need it. Let's check whether that's the case
abs(ba/(ba+nba) - bna/(bna+nbna)) - abs(ba/(ba+bna) - nba/(nba+nbna))

Ok, we need smoothing if 

(ba+nba) == 0
(bna+nbna) == 0
(ba+bna) == 0
(nba+nbna) == 0

nbna is always going to be > 0 if we're dealing with any real world dataset (it would be absurd if every utterance in the dataset used the same marker):

(ba+nba) == 0
(ba+bna) == 0

are only true if a never says the marker. Since we're already filtering for that, we don't need smoothing for the regular probability subtraction.

What about for the logs? We need to worry about when 
ba == 0 and bna == 0
ba == 0 and nba == 0

and these are also being filtered.

We also need to take care of the case where ba\*nbna == 0 and nba\*bna == 0 (the shuffle example)
Since we know that nbna is also > 0.5 for our datasets we don't have to worry about this case.

On the off chance that we do encounter a dataset in which nbna <= 0.5, there is a simple solution. Since we only encounter NA when this happens, and this only happens when alignment is 0, we can manually set alignment to 0 when ba\*nbna == 0 and nba\*bna == 0

So we don't need smoothing!!!!!


Plots
---------------------

First let's check the completely shuffled dataset

```{r}
  df <- fread('debug/shuffled/TTTTTTTT.csv', header=T)
  
  df$alignment <- log(abs(df$ba/(df$ba+df$nba) - df$bna/(df$bna+df$nbna))) - log(abs(df$ba/(df$ba+df$bna) - df$nba/(df$nba+df$nbna)))
  
  df$alignment[is.nan(df$alignment)] <- 0
  
  ggplot(d, aes(y=alignment, x=paste(verifiedSpeaker, verifiedReplier))) + geom_violin()
  
  t.test(df$alignment~df$verifiedSpeaker)
```

Great, we aren't getting a corellation between alignment and power on the shuffled dataset.

Let's check with the unshuffled dataset:

```{r}
  df <- fread('debug/shuffled/FFFFFFFF_300.csv', header=T)
  
  df$alignment <- log(abs(df$ba/(df$ba+df$nba) - df$bna/(df$bna+df$nbna))) - log(abs(df$ba/(df$ba+df$bna) - df$nba/(df$nba+df$nbna)))
  
  df$alignment[is.nan(df$alignment)] <- 0
  
  ggplot(d, aes(y=alignment, x=paste(verifiedSpeaker, verifiedReplier))) + geom_violin()
  
  t.test(df$alignment~df$verifiedSpeaker)
```

This is disappointing...negative alignment. Let's check with a cutoff

```{r}
  cutoff = 5
  
  df <- fread('debug/shuffled/FFFFFFFF_300.csv', header=T)
  
  d <- df %>%
    filter((ba+nba)>=cutoff,(bna+nbna)>=cutoff,(ba+bna)>=cutoff,(nba+nbna)>=cutoff)
  
  d$alignment <- log(abs(d$ba/(d$ba+d$nba) - d$bna/(d$bna+d$nbna))) - log(abs(d$ba/(d$ba+d$bna) - d$nba/(d$nba+d$nbna)))
  
  d$alignment[is.nan(d$alignment)] <- 0
  
  ggplot(d, aes(y=alignment, x=paste(verifiedSpeaker, verifiedReplier))) + geom_violin()
  
  t.test(d$alignment~d$verifiedSpeaker)
```

Ok, it seems we're still not getting the expected results. 
Let's check the conversations that are generating extreme alignment results

Checking...

Hmm, it seems like a lot of these conversations should be discarded because they're someone replying multiple times to one tweet. And it looks like it's pretty pervasive - even tweets with < -0.125 alignmenthave multiple replies to one msg.

Could it be because of cutoffs? We still want the cutoff regardless. Is there any way to solve this problem? One thing we could do is group all the replies to one message into one tweet. We'd then want to change our formula to incoporate number of tokens. Let's try that

Math stuff...

Cool, we've got a formula for ba:

Am = # of times A says a marker in an utterance
Bm = # of times B says a marker in an utterance
At = # of tokens A says in an utterance
Bt = # of tokens B says in an utterance

aIntb = Am\*Bm/(At\*Bt)
aIntnb = Am\*(Bt-Bm)/(At\*Bt)

ba = aIntb/(aIntb + aIntnb)

How would this formula affect alignment?

Four cases (assuming A influences B):

1) A uses a lot of tokens, B uses a lot of tokens

```{r}
  Am = 10
  Bm = 5
  At = 100
  Bt = 100
  
  aIntb = Am*Bm/(At*Bt)
  aIntnb = Am*(Bt-Bm)/(At*Bt)
  
  ba = aIntb/(aIntb + aIntnb)
  
  ba
```
= 0.05
2) A uses a lot of tokens, B does not use a lot of tokens

```{r}
  Am = 10
  Bm = 5
  At = 100
  Bt = 50
  
  aIntb = Am*Bm/(At*Bt)
  aIntnb = Am*(Bt-Bm)/(At*Bt)
  
  ba = aIntb/(aIntb + aIntnb)
  
  ba
```
= 0.1

3) 4) A does not use a lot of tokens, B uses a lot of tokens
```{r}
  Am = 10
  Bm = 5
  At = 50
  Bt = 100
  
  aIntb = Am*Bm/(At*Bt)
  aIntnb = Am*(Bt-Bm)/(At*Bt)
  
  ba = aIntb/(aIntb + aIntnb)
  
  ba
```
= 0.05

4) A does not use a lot of tokens, B does not use a lot of tokens
```{r}
  Am = 10
  Bm = 5
  At = 50
  Bt = 50
  
  aIntb = Am*Bm/(At*Bt)
  aIntnb = Am*(Bt-Bm)/(At*Bt)
  
  ba = aIntb/(aIntb + aIntnb)
  
  ba
```
= 0.1

This is interesting... It seems that alignment ba only depends on the number of tokens B uses. 
And if B uses a lot of tokens, alignment decreases, which is exactly what we want!!!

Or do we?

Since we'll be aggregating over ba, should longer responses reduce alignment or should they just have less of a weightage on alignment?

Or, what seems like the best option, is to have a higher weightage on words initially spoken in an utterance, than to words spoken later in an tterance. What would something like this look like?

One thing we could is set B's prior words as A's words. Does this make sense though?

If we consider that every single word B utters should show some degree of alignment, and assuming every word (except the marker we're looking at) has a maximum alignment, then yes it does make sense. 

However, B might start self aligning...

Instead what we can do is P(B|A) - P(B|B's priors).

Alas, this might not work because we can't differentiate easily between alignment and self-alignment.

We seem to be on the right track though. Perhaps the solution is to set Bt as the tokens we've seen thus far in B's utterance, and Bm as the markers we've seen thus far.

Let's run some numbers

```{r}
  Am = 10
  Bm = 1
  At = 50
  Bt = 1
  
  aIntb = Am*Bm/(At*Bt)
  aIntnb = Am*(Bt-Bm)/(At*Bt)
  
  ba = aIntb/(aIntb + aIntnb)
  
  ba
```

If b says a marker immediately after A says the marker, we get an alignment of 1


```{r}
  Am = 10
  Bm = 1
  At = 50
  Bt = 3
  
  aIntb = Am*Bm/(At*Bt)
  aIntnb = Am*(Bt-Bm)/(At*Bt)
  
  ba = aIntb/(aIntb + aIntnb)
  
  ba
```

If b says a marker 3 words after A says the marker, we get an alignment of 1/3. Exactly as we expect

Averaging the two gives us 

```{r}
  (1+ 1/3)/2
```
a number closer to 1, because it seems like strong alignment. The formula seems like it's the best solution!

Our new formula should also ideally incorporate where A's markers are
Let's set Am/At as the average of Am/At, where we add a new element to the average starting from the end of A's utterance to the start.

e.g.

For the tweet pair

marker token token marker

token marker token token marker

```{r}
a = mean(c(1/1,2/4))


aIntb = a*(1/2)
aIntnb = a*(2-1)/(2)
ba1 = aIntb/(aIntb + aIntnb)
  
aIntb = a*(2/5)
aIntnb = a*(5-2)/(5)
ba2 = aIntb/(aIntb + aIntnb)

mean(c(ba1, ba2))

```

We should also consider if we should compute on all tokens, not just on all markers

marker token token marker

token marker token token marker

```{r}
a = mean(c(1/1, 1/2, 1/3, 2/4))

tokens = 1
markers = 0
aIntb = aIntb = a*(markers/tokens)
aIntnb = a*(tokens-markers)/(tokens)
ba1 = aIntb/(aIntb + aIntnb)

tokens = 2
markers = 1
aIntb = a*(markers/tokens)
aIntnb = a*(tokens-markers)/(tokens)
ba2 = aIntb/(aIntb + aIntnb)
  
tokens = 3
markers = 1
aIntb = a*(markers/tokens)
aIntnb = a*(tokens-markers)/(tokens)
ba3 = aIntb/(aIntb + aIntnb)

tokens = 4
markers = 1
aIntb = a*(markers/tokens)
aIntnb = a*(tokens-markers)/(tokens)
ba4 = aIntb/(aIntb + aIntnb)

tokens = 5
markers = 2
aIntb = a*(markers/tokens)
aIntnb = a*(tokens-markers)/(tokens)
ba5 = aIntb/(aIntb + aIntnb)

ba = mean(c(ba1, ba2, ba3, ba4, ba5))

ba

```

Hmm, including every token has a pretty strong effect.

Let's say B has a 100 word reply and only says the markers as the first word and as the last word. 

Averaging across all tokens gives us an alignemnt ~= 0

Averaging across markers gives us an alignment of 

```{r}
a = mean(c(1/1,2/4))


tokens = 1
markers = 1
aIntb = aIntb = a*(markers/tokens)
aIntnb = a*(tokens-markers)/(tokens)
ba1 = aIntb/(aIntb + aIntnb)

tokens = 100
markers = 2
aIntb = a*(markers/tokens)
aIntnb = a*(tokens-markers)/(tokens)
ba2 = aIntb/(aIntb + aIntnb)
  
mean(c(ba1, ba2))

```

Since we know that there is alignment, there is a case to be made for only averagin across markers. However, it still doesn't seem mathematically sound. 

The average words/utterance is at least 14. Therefore, if we average across all tokens, we should get

```{r}
(1/1 + 1/2 + 1/3 + 1/4 + 1/5 + 1/6 + 1/7 + 1/8 + 1/9 + 1/10 + 1/11 + 1/12 + 1/13 + 1/14)/14

```
which is 0.2322 - not that bad of a number. 

Let's also try where the marker is in the middle of the utterance

```{r}
(0/1 + 0/2 + 0/3 + 0/4 + 0/5 + 0/6 + 1/7 + 1/8 + 1/9 + 1/10 + 1/11 + 1/12 + 1/13 + 1/14)/14

```

And now we get 0.05

In the middle just for comparison

```{r}
(0/1 + 0/2 + 0/3 + 0/4 + 1/5 + 1/6 + 1/7 + 1/8 + 1/9 + 1/10 + 1/11 + 1/12 + 1/13 + 1/14)/14

```

0.083444, so it seems that the position of the marker has a huge impact. Let's use the averaging by tokens for now. 

Putting it all together
------------------------

Therefore, our two formulas are

a = mean(marker/tokens)
aIntb = a*(markers/tokens)
aIntnb = a*(tokens-markers)/(tokens)
ba = mean(aIntb/(aIntb + aIntnb))
nba = mean(aIntnb/(aIntnb + aIntb))

a = mean((tokens-marker)/tokens)
aIntb = a*(markers/tokens)
aIntnb = a*(tokens-markers)/(tokens)
bna = mean(aIntb/(aIntb + aIntnb))
nbna = mean(aIntnb/(aIntnb + aIntb))

log(abs(d$ba/(d$ba+d$nba) - d$bna/(d$bna+d$nbna))) - log(abs(d$ba/(d$ba+d$bna) - d$nba/(d$nba+d$nbna)))

Let's check it out!

```{r}
  df <- fread("debug/results_50.csv")

  ggplot(df, aes(x=paste(verifiedSpeaker, verifiedReplier), y=alignment)) + geom_violin() +  stat_summary(geom="point", fun.y="mean")
  
  d <- filter(df, verifiedSpeaker==T&verifiedReplier==F)
  mean(d$alignment)
  d <- filter(df, verifiedSpeaker==F&verifiedReplier==T)
  mean(d$alignment)
  d <- filter(df, verifiedSpeaker==T&verifiedReplier==T)
  mean(d$alignment)
  d <- filter(df, verifiedSpeaker==F&verifiedReplier==F)
  mean(d$alignment)
```

Still not seeing expected results...

Here's the problem. 

P(A|nB utterance): Afreq = 1
P(B|nA utterance): Bfreq = 0
P(A|B utterance): Afreq = 0.9
P(B|A utterance): Bfreq = 0.5


P(A|nB utterance): Afreq = 0.1
P(B|nA utterance): Bfreq = 0
P(A|B utterance): Afreq = 0.099999
P(B|A utterance): Bfreq = 0.99

With our current formula, the top example would be given a higher alignment difference because 

P1(A|B) - P1(B|A) >> P2(B|A) - P2(A|B)

We should instead have been calculating percent differences by doing

(P1(A|B) - P1(B|A))/((P1(A|B)+P1(B|A))/2) = (0.5 - 0.1)/0.3 = 1.3333

(P2(A|B) - P2(B|A))/((P2(A|B)+P2(B|A))/2) = (0.99 - (0.099999-0.1))/((0.99 + (0.099999-0.1))/2) = 2.000004

Let's try it out 

After plotting...


Hmm, it seems like we're still seeing unexpected results. Let's take a look at the data

One of the smallest alignment values is:

548141920741244928	100107343	Today is Christmas ?	548142177302614016	69592091	[MENTION] no its jesus ' birthday

Let's add a cutoff that we have to have seen the marker in at least 5 times (doesn't matter if a says it or b says it)
```{r}
  df <- fread("debug/results_50.csv") %>%
    filter(numMarkers >= 5)

  ggplot(df, aes(x=paste(verifiedSpeaker, verifiedReplier), y=alignment)) + geom_violin() +  stat_summary(geom="point", fun.y="mean")
  
  d <- filter(df, verifiedSpeaker==T&verifiedReplier==F)
  mean(d$alignment)
  d <- filter(df, verifiedSpeaker==F&verifiedReplier==T)
  mean(d$alignment)
  d <- filter(df, verifiedSpeaker==T&verifiedReplier==T)
  mean(d$alignment)
  d <- filter(df, verifiedSpeaker==F&verifiedReplier==F)
  mean(d$alignment)
```

It seems we're still getting unexpected results. Let's take a look

TRUE/FALSE

1) Positive with lot's of exclamations - negative with no exclamations
  sh ./grepper.sh 140519774 57175268 \!
  -1.995019
  
2) Used a lot of quotes with colons, tweeted opinions (thus didn't use colons)
  sh ./grepper.sh 40981798 2648534065 \:
  -1.991485
  
3) Someone that just replies "follow me""
  sh ./grepper.sh 163730859 1163311250 a
  -1.981122
  
4) Legitimate divergence
  sh ./grepper.sh 180949358 2854184952 \!
  -1.993327
  
5) Legitimate divergence
  sh ./grepper.sh 310072711 2626722111 \.
  -1.983079
  
6) Legitimate divergence
  sh ./grepper.sh 39596022 2286250129 a
  -1.987811
  
7) Legitimate divergence
  sh ./grepper.sh 30495613 70134737 \.
  -1.975477
  
8) Legitimate divergence
  sh ./grepper.sh 21111883 2900058128 \.
  -1.973785
  
9) Legitimate divergence
  sh ./grepper.sh 100107343 2274086636 \.
  -1.964990
  
10) Msg tweets what another person said, replier comments about the tweet
  sh ./grepper.sh 13524182 1365592591 \:
  -1.966488
  
6/10 are legitimate divergences

Let's try FALSE/TRUE

1) Legitimate divergence
  sh ./grepper.sh 1048642166 1561889707 i
  -1.935103
  
2) Question mark
  sh ./grepper.sh 17837051 14608335 \?
  -1.950964
  
3) One exclamation mark msg, three exclamation mark reply
  sh ./grepper.sh 461410856 40981798 \!
  -1.952903
  
4) Legitimate divergence
  sh ./grepper.sh 2236787785 31611298 a
  -1.918316

Maybe we should look at by people, instead of treating categories as their own things.

```{r}
  df <- fread("debug/results_50.csv") %>%
    group_by(msgUserId, replyUserId, verifiedSpeaker, verifiedReplier) %>%
    summarize(alignment=mean(alignment))
  
  d <- df[with(df, order(-alignment)), ]
  ggplot(df, aes(x=paste(verifiedSpeaker, verifiedReplier), y=alignment)) + geom_violin() +  stat_summary(geom="point", fun.y="mean")
  
  d <- filter(df, verifiedSpeaker==T&verifiedReplier==F)
  mean(d$alignment)
  d <- filter(df, verifiedSpeaker==F&verifiedReplier==T)
  mean(d$alignment)
  d <- filter(df, verifiedSpeaker==T&verifiedReplier==T)
  mean(d$alignment)
  d <- filter(df, verifiedSpeaker==F&verifiedReplier==F)
  mean(d$alignment)
```

Nope. Perhaps we should take a look at max alignlment of utterances

```{r}
  df <- fread("debug/results_50.csv") %>%
    filter(numMarkers >= 5) %>%
    group_by(msgUserId, replyUserId, verifiedSpeaker, verifiedReplier) %>%
    summarize(alignment=max(alignment))
  
  d <- df[with(df, order(-alignment)), ]
  ggplot(df, aes(x=paste(verifiedSpeaker, verifiedReplier), y=alignment)) + geom_violin() +  stat_summary(geom="point", fun.y="mean")
  
  d <- filter(df, verifiedSpeaker==T&verifiedReplier==F)
  mean(d$alignment)
  d <- filter(df, verifiedSpeaker==F&verifiedReplier==T)
  mean(d$alignment)
  d <- filter(df, verifiedSpeaker==T&verifiedReplier==T)
  mean(d$alignment)
  d <- filter(df, verifiedSpeaker==F&verifiedReplier==F)
  mean(d$alignment)
```

Interesting...now TrueFalse takes the lead.

```{r}
  df <- fread("debug/shuffled/TTTTTTTT_50.csv") %>%
    filter(numMarkers >= 5) %>%
    group_by(msgUserId, replyUserId, verifiedSpeaker, verifiedReplier) %>%
    summarize(alignment=max(alignment))
  
  d <- df[with(df, order(-alignment)), ]
  ggplot(df, aes(x=paste(verifiedSpeaker, verifiedReplier), y=alignment)) + geom_violin() +  stat_summary(geom="point", fun.y="mean")
  
  d <- filter(df, verifiedSpeaker==T&verifiedReplier==F)
  mean(d$alignment)
  d <- filter(df, verifiedSpeaker==F&verifiedReplier==T)
  mean(d$alignment)
  d <- filter(df, verifiedSpeaker==T&verifiedReplier==T)
  mean(d$alignment)
  d <- filter(df, verifiedSpeaker==F&verifiedReplier==F)
  mean(d$alignment)
```

The alignment values are smaller, but stil very positive. We're of course forgetting about divergence.


```{r}
  df <- fread("debug/shuffled/TTTTTTTT_50.csv") %>%
    filter(numMarkers >= 5) %>%
    group_by(msgUserId, replyUserId, verifiedSpeaker, verifiedReplier) %>%
    summarize(alignment=max(alignment)+min(alignment))
  
  d <- df[with(df, order(-alignment)), ]
  ggplot(df, aes(x=paste(verifiedSpeaker, verifiedReplier), y=alignment)) + geom_violin() +  stat_summary(geom="point", fun.y="mean")
  
  d <- filter(df, verifiedSpeaker==T&verifiedReplier==F)
  mean(d$alignment)
  d <- filter(df, verifiedSpeaker==F&verifiedReplier==T)
  mean(d$alignment)
  d <- filter(df, verifiedSpeaker==T&verifiedReplier==T)
  mean(d$alignment)
  d <- filter(df, verifiedSpeaker==F&verifiedReplier==F)
  mean(d$alignment)
```

Cool, exactly what we expect. Let's try it out on non-shuffled data

```{r}
  df <- fread("debug/results_50.csv") %>%
    filter(numMarkers >= 5) %>%
    group_by(msgUserId, replyUserId, verifiedSpeaker, verifiedReplier) %>%
    summarize(alignment=max(alignment)+min(alignment))
  
  d <- df[with(df, order(-alignment)), ]
  ggplot(df, aes(x=paste(verifiedSpeaker, verifiedReplier), y=alignment)) + geom_violin() +  stat_summary(geom="point", fun.y="mean")
  
  d <- filter(df, verifiedSpeaker==T&verifiedReplier==F)
  d <- filter(d, alignment<mean(d$alignment)-2*sd(d$alignment)|alignment>mean(d$alignment)+2*sd(d$alignment))
  mean(d$alignment)
  d <- filter(df, verifiedSpeaker==F&verifiedReplier==T)
  d <- filter(d, alignment<mean(d$alignment)-2*sd(d$alignment)|alignment>mean(d$alignment)+2*sd(d$alignment))
  mean(d$alignment)
  d <- filter(df, verifiedSpeaker==T&verifiedReplier==T)
  d <- filter(d, alignment<mean(d$alignment)-2*sd(d$alignment)|alignment>mean(d$alignment)+2*sd(d$alignment))
  mean(d$alignment)
  d <- filter(df, verifiedSpeaker==F&verifiedReplier==F)
  d <- filter(d, alignment<mean(d$alignment)-2*sd(d$alignment)|alignment>mean(d$alignment)+2*sd(d$alignment))
  mean(d$alignment)
```

So there's a problem with how we're calculating the formula.

...Fixing formula...

```{r}
  cutoff = 5
  df <- fread("debug/results_50.csv") %>%
    filter((ba+nba)>=cutoff,(bna+nbna)>=cutoff) %>%
    filter(ba!=0)
  
  df$a = df$ba/(df$ba+df$nba)
  df$alignment =  log(df$ba+df$a)-log(df$ba+df$nba+2*df$a)-log(df$bna+df$a)+log(df$bna+df$nbna+2*df$a)
  
  df <- df %>%
    filter(verifiedReplier==F,(ba+nba)>=cutoff,(bna+nbna)>=cutoff) %>%
    group_by(verifiedSpeaker,verifiedReplier,category) %>%
    summarize(prop=mean((2*ba+nba+bna)/(2*(ba+nba+bna+nbna))),alignment=mean(alignment))
      
  ggplot(df, aes(x=verifiedSpeaker, y=alignment)) + geom_violin() + stat_summary(geom="point", fun.y="mean")
  d <- df %>%
    filter(msgUserId==813286)
  d[with(d, order(-alignment)), ]
```

By category 
```{r}
  cutoff = 5
  df <- fread("debug/results_50.csv") %>%
    filter((ba+nba)>=cutoff,(bna+nbna)>=cutoff) %>%
    filter(ba > 0) %>%
    filter(verifiedReplier==F)
  df$a = df$ba/(df$ba+df$nba)
  df$alignment =  log(df$ba+df$a)-log(df$ba+df$nba+2*df$a)-log(df$bna+df$a)+log(df$bna+df$nbna+2*df$a)
  
  df <- df %>%
    group_by(category, verifiedSpeaker) %>%
    summarize(alignment=mean(alignment), convs=n())
  ggplot(df, aes(x=verifiedSpeaker, y=alignment)) + geom_violin() + stat_summary(geom="point", fun.y="mean") + geom_text(aes(label=category,color=convs),size=4.5)
  
```

By sentiment 
```{r}
  cutoff = 5
  df <- fread("debug/results_50.csv") %>%
    filter(ba > 0)
  df$msgSentiment <- df$msgSentiment>0
  df$replySentiment <- df$replySentiment> 0
  df$a = df$ba/(df$ba+df$nba)
  df$alignment =  log(df$ba+df$a)-log(df$ba+df$nba+2*df$a)-log(df$bna+df$a)+log(df$bna+df$nbna+2*df$a)
  
  ggplot(df, aes(x=paste(msgSentiment, replySentiment), y=alignment)) + geom_violin() + stat_summary(geom="point", fun.y="mean")
  
```
